{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ECE 57000 Assignment 6 Exercise"
      ],
      "metadata": {
        "id": "d7uflfemp7XR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your Name:"
      ],
      "metadata": {
        "id": "vr0qq_YHqHDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this assignment, you will explore various density estimation methods."
      ],
      "metadata": {
        "id": "mGQSwz_MqiI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Density Estimatino in 1D (50/100 points)\n",
        "In this exercise, you will write code to estimate 1D densities. Specifically, you will write code to estimate a Guassian Density, a Histogram Density, and a Kernel Density."
      ],
      "metadata": {
        "id": "5bsYy6MzqKhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.1: Guassian Distribution (5/100 points)\n",
        "At first, we start with defining Gaussian PDF. This function will be used througout this assignment.\n",
        "\n",
        "Gaussian distribution is defined as\n",
        "$$pdf (x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\Bigl( - \\frac{(x -\\mu)^2}{2\\sigma^2} \\Bigr)$$\n",
        "with a mean $\\mu$ and variance $\\sigma^2$."
      ],
      "metadata": {
        "id": "ASKaAuoarBGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def gaussian_pdf(x, mean, variance):\n",
        "    \"\"\"\n",
        "    Compute the Gaussian Probability Density Function (PDF).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : float or array-like\n",
        "        The point(s) at which to evaluate the Gaussian PDF.\n",
        "    mean : float\n",
        "        The mean (center) of the Gaussian distribution.\n",
        "    variance : float\n",
        "        The variance (spread) of the Gaussian distribution.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pdf : float or array-like\n",
        "        The probability density of the Gaussian distribution evaluated at `x`.\n",
        "        If `x` is an array, returns an array of the same shape with PDF values.\n",
        "    \"\"\"\n",
        "    ##### Your code here #####\n",
        "    # You should make the probability density of the Gaussian distribution using numpy.\n",
        "\n",
        "\n",
        "\n",
        "    ##########################\n",
        "\n",
        "    return pdf\n",
        "\n",
        "# Set parameters for the Gaussian distribution\n",
        "mu = 1.5\n",
        "sigma = 0.5\n",
        "variance = sigma ** 2\n",
        "\n",
        "# Generate sample data\n",
        "np.random.seed(42)\n",
        "X = np.random.normal(mu, sigma, size=1000)\n",
        "\n",
        "# Define range for x values for plotting\n",
        "x = np.linspace(X.min(), X.max(), 1000)\n",
        "\n",
        "# Compute the Gaussian PDF for the range of x values\n",
        "pdf_values = gaussian_pdf(x, mu, variance)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(x, pdf_values, label='Gaussian PDF', color='blue')\n",
        "plt.hist(X, bins=30, density=True, alpha=0.6, color='gray', label='Sample Data Histogram')\n",
        "plt.title(\"Gaussian PDF with Mean = 1.5 and Variance = 0.5\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qv-7l2XbFHtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.2: Guassian Density (15/100 points)\n",
        "For this assignment, you will estimate a Guassian Density via MLE. As discussed in class, Density Estimation finds a density (PDF/PMF) that represents the data well. The goal is finding a density/distribution function $\\hat{P}(x)$ that as close to a ground-truth distribution $P(x)$ as possible. This simplifies to estimating the mean and standard deviation of the data and using these empricial estimates for the Gaussian distribution.\n"
      ],
      "metadata": {
        "id": "vb7tZvuBDo3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator\n",
        "np.random.seed(42)\n",
        "class GaussianDensity(BaseEstimator):\n",
        "    def fit(self, X, y=None):\n",
        "        ##### Your code here #####\n",
        "        # You should estimate the mean and std of the data  and save as self.mean_and self.std\n",
        "        # (note that X will be shape (n,1) because there is only 1 feature).\n",
        "\n",
        "\n",
        "        ##########################\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        ##### Your code here #####\n",
        "        # This should return the PDF values for each sample in X (again of shape (n, 1))\n",
        "        # This should use your self.mean and self.std variables saved from the fit method\n",
        "\n",
        "        ##########################\n",
        "        return pdf_values  # Output should be of shape (n,), i.e., a 1D array\n",
        "\n"
      ],
      "metadata": {
        "id": "6BPv6ZkFqGjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.3: Histogram density (15/100 points)\n",
        "Now you will implement a histogram density estimate given min, max and number of bins.\n",
        "The function [`np.searchsorted`](https://numpy.org/doc/stable/reference/generated/numpy.searchsorted.html) may be useful but is not required. Additional instructions are inline in the code template below."
      ],
      "metadata": {
        "id": "T5V8ZaUcMpRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator\n",
        "np.random.seed(42)\n",
        "class HistogramDensity(BaseEstimator):\n",
        "    def __init__(self, n_bins, min_val, max_val):\n",
        "        self.n_bins = n_bins\n",
        "        self.min_val = min_val\n",
        "        self.max_val = max_val\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        ##### Your code here #####\n",
        "        # First create equally spaced bin_edges based on min_val, max_val and n_bins\n",
        "        #  and save as self.bin_edges_\n",
        "        #  (note the shape of self.bin_edges_ should be (n_bins+1,) )\n",
        "        # Second, estimate the frequency for each bin based on the input data X\n",
        "        #  (i.e., the number of training samples that fall into that bin divided\n",
        "        #  by the total number of samples)\n",
        "        # Third, using the probability for each bin, compute the density value (i.e., PDF) for\n",
        "        #  each bin. (Note you will have to account for the width of the bin to ensure\n",
        "        #  that integrating your density function from min_value to max_value will be 1).\n",
        "        #  Save the density per bin as self.pdf_per_bin_ which should have the shape (n_bins,)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##########################\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        ##### Your code here #####\n",
        "        # You should return the PDF value of the samples X.  This requires finding out which\n",
        "        #  bin each sample falls into and returning it's corresponding density value\n",
        "        #  **Importantly, if the value is less than min_value or greater than max_value,\n",
        "        #    then a pdf value of 0 should be returned.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return pdf_values  # Output should be of shape (n,), i.e., a 1D array\n",
        "        ##########################"
      ],
      "metadata": {
        "id": "Go1jjZEvp651"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.4: Kernel density (15/100 points)\n",
        "Now you will implement a kernel density estimate (KDE) via a Gaussian kernel given the bandwidth parameter (i.e., the standard deviation of the Gaussian kernel.\n",
        "Specifically, the Gaussian kernel density is given by:\n",
        "$$p(x; D) = \\frac{1}{n}\\sum_{i=1}^n p_{N}(x; \\mu = x_i, \\sigma=h) $$\n",
        "where $D=\\{x_i\\}_{i=1}^n$ is a training dataset of $n$ samples, $p_{N}$ is the Gaussian/normal density function and $h$ is called the bandwidth hyperparameter of the KDE model.\n",
        "(Note that fitting merely requires saving the training dataset. The saved training data is then used at test time to compute the densities of new samples.)"
      ],
      "metadata": {
        "id": "eP1rHet4MtQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator\n",
        "np.random.seed(42)\n",
        "class KernelDensity(BaseEstimator):\n",
        "    def __init__(self, bandwidth):\n",
        "        self.bandwidth = bandwidth\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        ##### Your code here #####\n",
        "        # Save the training data in self.X_train_\n",
        "\n",
        "\n",
        "\n",
        "        ##########################\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        ##### Your code here #####\n",
        "        # You should return the KDE PDF value of the samples X.\n",
        "        #  Note that the mean above is over the TRAINING samples, not the test samples\n",
        "        #  so you should use the samples saved by the fit method.\n",
        "        # Estimate KDE PDF values for samples in X\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##########################\n",
        "        return pdf_values  # Output should be of shape (n,), i.e., a 1D array"
      ],
      "metadata": {
        "id": "DAOjJ6xNMuzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You must run the testing code below for your density estimators."
      ],
      "metadata": {
        "id": "r7DFj-B8M4jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %pdb on\n",
        "import scipy.stats\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Generate some data and split into train and test\n",
        "np.random.seed(42) # Fix random seed\n",
        "min_val, max_val = -5, 5\n",
        "diff = max_val - min_val\n",
        "X = diff * np.vstack([scipy.stats.beta(6,1).rvs(size=(300,1)), scipy.stats.beta(2,7).rvs(size=(100,1))]) - diff/2\n",
        "X_train, X_test = train_test_split(X, test_size=0.5, random_state=15)\n",
        "print(X_train.shape, X_test.shape)\n",
        "\n",
        "# Loop through models\n",
        "models = [GaussianDensity(),\n",
        "          HistogramDensity(n_bins=15, min_val=min_val, max_val=max_val),\n",
        "          KernelDensity(bandwidth=1)\n",
        "         ]\n",
        "for model in models:\n",
        "    print(f'Fitting {type(model).__name__} model')\n",
        "    # Fit models\n",
        "    model.fit(X_train)\n",
        "\n",
        "    # Sanity checks\n",
        "    xq = np.linspace(min_val-diff, max_val+diff, num=1000)\n",
        "    pdf_vals = model.predict_proba(xq.reshape(-1, 1))\n",
        "    # Adjust shape if model is GaussianDensity and output has two dimensions (N, 1)\n",
        "    if isinstance(model, GaussianDensity) and pdf_vals.shape[1] == 1:\n",
        "        pdf_vals = pdf_vals.reshape(-1)  # Flatten to shape (N,)\n",
        "    # Check that right size and >= 0\n",
        "    print(f'{len(pdf_vals.shape) == 1 and pdf_vals.shape[0] == len(xq)}, Shape={pdf_vals.shape}'\n",
        "          f' - Is the output the correct shape?')\n",
        "    print(f'{np.all(pdf_vals>=0)}, Num neg={np.sum(pdf_vals < 0)} - Are all pdf values >= 0? ')\n",
        "\n",
        "    # Check that integrates to 1 vai approximate numerical integration\n",
        "    model_pdf = lambda x: model.predict_proba(np.array(x).reshape(1,1))[0]\n",
        "    quad_out = scipy.integrate.quad(model_pdf, min_val - diff, max_val + diff, limit=100, full_output=True)\n",
        "    # print(f'{np.abs(quad_out[0] - 1) < 1e-4}, quad_out={quad_out[0]} - Does the PDF integrate to 1? ')\n",
        "    print(f'quad_out={quad_out[0]}')\n",
        "    print('')\n",
        "\n",
        "    # Plot density model\n",
        "    plt.plot(xq, pdf_vals, label=type(model).__name__)\n",
        "\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "ZLXBzMnGM3_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Gaussian Mixture Model and EM Alogirhtm (50/100 points)\n",
        "In this exercise, you will implement a Gaussian Mixture Model (GMM) using the Expectation-Maximization (EM) algorithm. Specifically:\n",
        "- **Create a Ground-Truth GMM**: Define a Gaussian Mixture with specified parameters (e.g., number of components, means, variance, and weight) to serve as the \"ground truth.\"\n",
        "- **Approximate the GMM Using the EM Algorithm**: Initialize your Gaussian Mixture Model randomly from given data and apply the EM algorithm to approximate the parameters of the GMM based on the generated data."
      ],
      "metadata": {
        "id": "uXFkI_o8OFy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a ground truth and visualize it."
      ],
      "metadata": {
        "id": "z1PQn0BUOw7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from scipy.stats import norm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "n_samples = 100\n",
        "mu1, sigma1 = -5, 1.2\n",
        "mu2, sigma2 = 5, 1.8\n",
        "mu3, sigma3 = 0, 1.6\n",
        "\n",
        "def plot_pdf(mu,sigma,label,alpha=0.5,linestyle='k--',density=True,color='green',use_label=False):\n",
        "    \"\"\"\n",
        "    Plot 1-D Guassian Distribution and its PDF curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array-like, shape (n_samples,)\n",
        "        The input data.\n",
        "    \"\"\"\n",
        "    # Create Guassian Distribution data by specifying mean and variance.\n",
        "    np.random.seed(42)\n",
        "    X = np.random.normal(mu, sigma, size=1000)\n",
        "\n",
        "    # Draw histogram of Gaussian Distribution\n",
        "    if use_label:\n",
        "        plt.hist(X, bins=50, density=density, alpha=alpha,label=label,color=color)\n",
        "    else :\n",
        "        plt.hist(X, bins=50, density=density, alpha=alpha, color=color)\n",
        "\n",
        "    # Plot the PDF\n",
        "    x = np.linspace(X.min(), X.max(), 1000)\n",
        "    y = gaussian_pdf(x, mu, sigma)\n",
        "    plt.plot(x, y, linestyle)\n",
        "plot_pdf(mu1,sigma1,label=r\"$\\mu={} \\ ; \\ \\sigma={}$\".format(mu1,sigma1),color='C0',use_label=True)\n",
        "plot_pdf(mu2,sigma2,label=r\"$\\mu={} \\ ; \\ \\sigma={}$\".format(mu2,sigma2),color='C1',use_label=True)\n",
        "plot_pdf(mu3,sigma3,label=r\"$\\mu={} \\ ; \\ \\sigma={}$\".format(mu3,sigma3),color='C2',use_label=True)\n",
        "plt.title(\"Ground Truth Gaussian Distribution\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9LkSMQDIQfx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we create the data $X$ consist of Gaussian Distributions as a synthetic dataset. However, note that we don’t know which component generated each point in $X$, nor do we know the exact parameters of these Gaussians in practice."
      ],
      "metadata": {
        "id": "3QrJdiM1SJca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a dataset\n",
        "np.random.seed(0)\n",
        "x1 = np.random.normal(loc = mu1, scale = np.sqrt(sigma1), size = n_samples)\n",
        "x2 = np.random.normal(loc = mu2, scale = np.sqrt(sigma2), size = n_samples)\n",
        "x3 = np.random.normal(loc = mu3, scale = np.sqrt(sigma3), size = n_samples)\n",
        "\n",
        "X = np.concatenate((x1,x2,x3))\n",
        "# Shuffle the order of X\n",
        "np.random.shuffle(X)\n",
        "print(X.shape)"
      ],
      "metadata": {
        "id": "bwzeGMjESDm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.1: Random Initialization (10/100 points)\n",
        "In this task, you will initialize mean, covariance, and weight parameters.\n",
        "1. Mean (μ): Initialize randomly by choosing random $N$ samples from $X$. The chosen data points serve as initial means for each component.\n",
        "2. Covariance (Σ): Initialize random numbers between 0 and 1 for each component.\n",
        "3. weight (mixing coefficients) (π): fraction per class refers to the likelihood that a particular data point belongs to each class. In the beginning, this will be equal for all clusters. Assume that we fit a GMM with three components. In this case weight parameter might be set to 1/3 for each component, resulting in a probability distribution of (1/3, 1/3, 1/3).\n",
        "\n",
        "\n",
        "***Hint***: Use ```np.random.choice``` and ```np.random.random_sample```.\n",
        "\n",
        "If you define means, variance, and pi in correct order, the result should be\n",
        "```\n",
        "means= [-5.38111866  2.92747209  3.46050925]\n",
        "variances= [0.98394927 0.50901966 0.7871809 ]\n",
        "pi= [0.33333333 0.33333333 0.33333333]\n",
        "```"
      ],
      "metadata": {
        "id": "GuWRUpnpWvum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_init(X, n_compenents):\n",
        "    \"\"\"\n",
        "    Initialize means, weights and variance randomly.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array-like, shape (n_samples,)\n",
        "        The input data.\n",
        "    n_components: integer number\n",
        "                  The number of Gaussian components.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    means : array-like, shape (n_components,)\n",
        "        Randomly chosen samples as initial means.\n",
        "    variances : array-like, shape (n_components,)\n",
        "        Random float numbers between [0,1] as initial variances.\n",
        "    pi : array-like, shape (n_components,)\n",
        "        Initial weight between components.\n",
        "    \"\"\"\n",
        "    ##### Your code here #####\n",
        "    # You should estimate choose three random data points in data to serves as 'means',\n",
        "    # three random float numbers between [0,1] as 'variances',\n",
        "    # and initial weight 'pi' giving equal weight to each component.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ##########################\n",
        "    print('means=',means)\n",
        "    print('variances=',variances)\n",
        "    print('pi=',pi)\n",
        "    plot_pdf(means[0],variances[0],'Random Init 01',color='C0',use_label=True)\n",
        "    plot_pdf(means[1],variances[1],'Random Init 02',color='C1',use_label=True)\n",
        "    plot_pdf(means[2],variances[2],'Random Init 03',color='C2',use_label=True)\n",
        "\n",
        "    plt.title(\"Random Initialization\")\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return means,variances,pi\n",
        "np.random.seed(24)\n",
        "n_compenents = 3\n",
        "means,variances,pi = random_init(X, n_compenents)"
      ],
      "metadata": {
        "id": "s6PB5HZITC0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.2: Expectation Step (E step) (20/100 points)\n",
        "For each data point $x_i$, calculate the probability that the data point belongs to cluster ($k$) using the below equation. $k$ is the number of distributions we are supposed to find.\n",
        "$$ r_{i,k} = \\frac{\\pi_k N (x_i \\vert \\mu_k, {\\scriptsize\\sum}_c )}{\\sum_{k=1}^K\\pi_k N (x_i \\vert \\mu_k, {\\scriptsize\\sum}_k )}$$\n",
        "where $\\pi_c$ is the mixing coefficient (weight) for the Guassian distribution $c$, which was initialized in the previous stage. $N(x\\vert\\mu,{\\scriptsize\\sum})$ descreibes the probability density function (PDF) of a Gaussian distribution with mean $\\mu$ and covariance Σ with respect to data point $x$."
      ],
      "metadata": {
        "id": "XGzoj3Ykvpha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def step_expectation(X,n_components,means,variances,pi):\n",
        "    \"\"\"\n",
        "    E Step - Calculate the responsibility matrix.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array-like, shape (n_samples,)\n",
        "        The data.\n",
        "    n_components : int\n",
        "        The number of clusters.\n",
        "    means : array-like, shape (n_components,)\n",
        "        The means of each mixture component.\n",
        "    variances : array-like, shape (n_components,)\n",
        "        The variances of each mixture component.\n",
        "    pi : array-like, shape (n_components,)\n",
        "        The mixing weights of each component.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    r : array-like, shape (n_components, n_samples)\n",
        "        The responsibility matrix, where responsibilities[j, i] is the responsibility of component j for data point i.\n",
        "    \"\"\"\n",
        "    n_samples = len(X)\n",
        "    r = np.zeros((n_components, n_samples))\n",
        "\n",
        "    ##### Your code here #####\n",
        "    # Calculate responsibilities for each component. Hint: Utilize 'gaussian_pdf' defined previously.\n",
        "    # After that, normalize responsibilities to sum to 1 for each sample\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ##########################\n",
        "\n",
        "    return r\n",
        "# Test Expectatino Step\n",
        "r = step_expectation(X,n_compenents,means,variances,pi)\n",
        "\n",
        "\n",
        "# Check if any summation values deviate from 1\n",
        "responsibility_sum = np.sum(r, axis=0)\n",
        "non_one_sums = np.isclose(responsibility_sum, 1, atol=1e-6)  # Use a small tolerance for floating-point precision\n",
        "print('Summation of responsibility:', responsibility_sum)\n",
        "print('Shape of responsibility:', r.shape) # It should be (3,300)\n",
        "if not np.all(non_one_sums):\n",
        "    print(\"Warning: Summation of responsibility includes non-one values.\")\n",
        "    print(\"Non-one summation values:\", responsibility_sum[~non_one_sums])\n",
        "else:\n",
        "    print(\"All summation values are approximately equal to 1.\")"
      ],
      "metadata": {
        "id": "niZzgBdGQmQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.3: Maximization Step (M step) (20/100 points)\n",
        "In this step, the algorithm uses the responsibilities of the Gaussian distributions (computed in the E-step) to update the estimates of the model's parameters.\n",
        "\n",
        "The M-step updates the estimates of the parameters as follows:    \n",
        "- $\\pi_k = \\frac{1}{N} \\sum_{i=1}^N r_{ik} = \\frac{N_k}{N}$ with $N_k = \\sum_{i=1}^N r_{ik}$.   \n",
        "- $\\mu_k =  \\frac{1}{N_k} \\sum_{i=1}^N r_{ik} x_i$.  \n",
        "- $\\sum_k =  \\frac{1}{N_k} \\sum_{i=1}^N r_{ik} (x_i - \\mu_k)(x_i - \\mu_k)^T$.  "
      ],
      "metadata": {
        "id": "CRmWNFLZ3em1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def step_maximization(X,r):\n",
        "    \"\"\"\n",
        "    M Step - Update parameters based on responsibilities.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array-like, shape (n_samples,)\n",
        "        The data.\n",
        "    r : array-like, shape (n_components, n_samples)\n",
        "        The responsibility matrix from the E-step.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    means : array-like, shape (n_components,)\n",
        "        Updated means of each mixture component.\n",
        "    variances : array-like, shape (n_components,)\n",
        "        Updated variances of each mixture component.\n",
        "    pi : array-like, shape (n_components,)\n",
        "        Updated mixing weights of each component.\n",
        "    \"\"\"\n",
        "    n_components, n_samples = r.shape\n",
        "\n",
        "    # # Initialize mean, variance, and pi.\n",
        "    means = np.zeros(n_components)\n",
        "    variances = np.zeros(n_components)\n",
        "    pi = np.zeros(n_components)\n",
        "\n",
        "\n",
        "    ##### Your code here #####\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ##########################\n",
        "    return means, variances, pi\n"
      ],
      "metadata": {
        "id": "hfWMHL533jbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define plotting function for GMM\n",
        "def plot_GMM(means,variances):\n",
        "    x = np.linspace(X.min(), X.max(), 1000)\n",
        "    y1 = gaussian_pdf(x, mu1, sigma1)\n",
        "    y2 = gaussian_pdf(x, mu2, sigma2)\n",
        "    y3 = gaussian_pdf(x, mu3, sigma3)\n",
        "    plt.plot(x, y1, 'r--')\n",
        "    plt.plot(x, y2, 'r--')\n",
        "    plt.plot(x, y3, 'r--')\n",
        "    color_gen = (x for x in ['C0','C1','C2'])\n",
        "    for mu,sigma in zip(means,variances):\n",
        "        plot_pdf(mu,sigma,alpha=0.5,label='d',color=next(color_gen),use_label=False)\n",
        "    plt.plot([], [], 'r--', label='Ground Truth')  # Red dashed line for Ground Truth\n",
        "    plt.plot([], [], 'k--', label='Gaussian Mixture Model')           # Black dashed line for GMM\n",
        "    # Show legend and plot\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "WuYa_MJlQ2L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You must run the testing code below for your GMM and EM algorithm.\n",
        "After first step, the result should be\n",
        "\n",
        "```\n",
        "means= [-4.23311078  1.80177309  3.64918194]\n",
        "variances= [0.98394927 0.50901966 0.7871809 ]\n",
        "pi= [0.41997062 0.16526334 0.41476604]\n",
        "```"
      ],
      "metadata": {
        "id": "0VtQLS6wCWnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps=50\n",
        "# Run GMM for n_steps\n",
        "for step in range(n_steps):\n",
        "    r = step_expectation(X,n_compenents,means,variances,pi)\n",
        "    means,varaiances,pi = step_maximization(X, r)\n",
        "    if step==0:\n",
        "        print(\"After First Step\\n\")\n",
        "        print('means=',means)\n",
        "        print('variances=',variances)\n",
        "        print('pi=',pi)\n",
        "plot_GMM(means,variances)"
      ],
      "metadata": {
        "id": "Dbe1SuJ49qPg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}