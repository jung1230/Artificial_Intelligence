{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#ECE 57000 Assignment 4 Exercise\n",
        "\n",
        "Name:"
      ],
      "metadata": {
        "id": "4Z5dTMmEipjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the following package for implementation."
      ],
      "metadata": {
        "id": "pfcdxFawlJTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "GBZdpuy_pcUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 1: Pisitional Encoding (20 points)"
      ],
      "metadata": {
        "id": "zB0SB2B4y5uj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 1. Implement Positional Encoding (15 points)\n",
        "\n",
        "Transformers process input sequences in parallel without an inherent understanding of word position, unlike recurrent neural networks (RNNs) that process sequences step by step. Positional encoding helps transformers gain this positional information by adding a vector to each word embedding that contains information about the word's position in the sequence. Specifically, positional encoding is a deterministic, fixed representation based on sine and cosine functions, and the formula for positional encoding is given as:\n",
        "$$\n",
        "\\mathrm{PE}(p o s, 2 i)=\\sin \\left(\\frac{p o s}{10000^{2 i / d_{\\text {model }}}}\\right),\n",
        "$$\n",
        "$$\n",
        "\\mathrm{PE}(p o s, 2 i+1)=\\cos \\left(\\frac{p o s}{10000^{2 i / d_{\\text {model }}}}\\right),\n",
        "$$\n",
        "where $pos$ is the position, $i$ is the dimension and $d_{\\text{model}}$ is the dimension of the model's embeddings.\n",
        "\n",
        "In practice, the positional encodings is added to the word embeddings, which is done by first creating a tensor of shape (1, sqeuence_length, d_model) and then adding it to the embeddings. Therefore, each dimension of the positional encoding corresponds to a sinusoid.\n",
        "\n",
        "If implemented correctly, the output should be like:\n",
        "\n",
        "Word Embeddings with Positional Encoding:\n",
        "\n",
        " tensor([[ 1.4000,  1.0479,  0.2396, -0.4558, -0.5485,  0.0267,  0.8648,  1.3756,\n",
        "          1.1994,  0.4603],\n",
        "        [ 1.4000,  1.0479,  0.2396, -0.4558, -0.5485,  0.0267,  0.8648,  1.3756,\n",
        "          1.1994,  0.4603],\n",
        "        [ 1.4000,  1.0479,  0.2396, -0.4558, -0.5485,  0.0267,  0.8648,  1.3756,\n",
        "          1.1994,  0.4603]], dtype=torch.float64)"
      ],
      "metadata": {
        "id": "juee9ETFlbF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding:\n",
        "    def __init__(self, d_model, max_len):\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "        self.positional_encoding = self._get_positional_encoding()\n",
        "\n",
        "    def _get_positional_encoding(self):\n",
        "        \"\"\"\n",
        "        Generate a matrix where each row corresponds to the positional encoding for a position.\n",
        "        \"\"\"\n",
        "        # <YOUR CODE HERE>\n",
        "\n",
        "        # <END YOUR CODE>\n",
        "\n",
        "    def get_encoding(self):\n",
        "        \"\"\"\n",
        "        Returns the positional encoding matrix.\n",
        "        \"\"\"\n",
        "        return self.positional_encoding\n",
        "\n",
        "def add_positional_encoding(word_embeddings, positional_encodings):\n",
        "    \"\"\"\n",
        "    Add positional encoding to word embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    - word_embeddings: (batch_size, seq_len, d_model) input embeddings\n",
        "    - positional_encodings: (seq_len, d_model) precomputed positional encodings\n",
        "\n",
        "    Returns:\n",
        "    - embeddings with positional encodings added.\n",
        "    \"\"\"\n",
        "    # <YOUR CODE HERE>\n",
        "\n",
        "    # <END YOUR CODE>\n",
        "\n",
        "# Test positional encoding on a sample embedding\n",
        "\n",
        "batch_size = 3\n",
        "seq_len = 10\n",
        "d_model = 128\n",
        "word_embeddings = torch.arange(1,129).repeat(batch_size, seq_len, 1).float()\n",
        "word_embeddings *= 0.1\n",
        "\n",
        "# Create Positional Encoding instance\n",
        "pos_enc = PositionalEncoding(d_model, seq_len)\n",
        "\n",
        "# Get positional encoding\n",
        "positional_encodings = pos_enc.get_encoding()\n",
        "\n",
        "# Add positional encodings to word embeddings\n",
        "encoded_embeddings = add_positional_encoding(word_embeddings, positional_encodings)\n",
        "\n",
        "print(\"Word Embeddings with Positional Encoding:\\n\", encoded_embeddings[:,:,3])"
      ],
      "metadata": {
        "id": "1JEqA3WTjdvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Visualize Positional Encoding (5 points)\n",
        "\n",
        "Generate and visualize the positional encodings using the positional_encoding function. You will use a heatmap to visualize how different positions and dimensions are represented by the encoding.\n",
        "\n"
      ],
      "metadata": {
        "id": "-MNoM7wNuwew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_positional_encoding(PE):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(PE, aspect='auto', cmap='viridis')\n",
        "    plt.colorbar(label=\"Encoding values\")\n",
        "    plt.xlabel(\"Embedding Dimension\")\n",
        "    plt.ylabel(\"Position in Sequence\")\n",
        "    plt.show()\n",
        "\n",
        "# Visualize for a sequence of length 64 and embedding size 32\n",
        "# <YOUR CODE HERE>\n",
        "\n",
        "# <END YOUR CODE>"
      ],
      "metadata": {
        "id": "WCgngo3Agz0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 2: Attention Score Function (30 points)"
      ],
      "metadata": {
        "id": "WbJLiwLNnxwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 1. Implement Scaled Dot-Product Attention (20 points)\n",
        "\n",
        "In this task, you will implement a function that computes scaled dot-product attention and use it to create a simple attention-based layer. The scaled dot-product attention computes the attention score by taking the dot product of queries $Q$ and keys $K$, scaling by the square root of the key dimension $d_k$, and applying a softmax to compute attention weights. These weights are then used to compute the weighted sum of the values $V$:\n",
        "$$\n",
        "\\operatorname{A}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V,\n",
        "$$\n",
        "where $Q, K \\in \\mathbb{R}^{L \\times d_k}$ and $V \\in \\mathbb{R}^{L \\times d_v}$. Notice that the softmax function is performed column-wise, and the output  of attention should be a matrix $A \\in \\mathbb{R}^{L \\times d_v}$.\n",
        "\n",
        "Furthermore, you will implement a batched version which takes into account several sequences at the same time, such that $Q, K \\in \\mathbb{R}^{N \\times L \\times d_k}$ and $V \\in \\mathbb{R}^{N \\times L \\times d_v}$ with $N$ the batch size. To do this, we recommend that you use `torch.bmm` for batched version of matrix multiplication for clarity.\n",
        "\n",
        "If implemented correctly, the output should be like:\n",
        "\n",
        "Attention output Shape: torch.Size([20, 5, 64])\n",
        "\n",
        "Attention Weights Shape: torch.Size([20, 5, 5])\n",
        "\n",
        "Is the implementation close to PyTorch implementation? True\n"
      ],
      "metadata": {
        "id": "DoXzr6Kgvzls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def att_score(Q, K, V):\n",
        "    \"\"\"\n",
        "    Computes scaled dot-product attention.\n",
        "\n",
        "    Parameters:\n",
        "    Q (torch.Tensor): Query matrix of shape (batch_size, num_heads, seq_len, d_k)\n",
        "    K (torch.Tensor): Key matrix of shape (batch_size, num_heads, seq_len, d_k)\n",
        "    V (torch.Tensor): Value matrix of shape (batch_size, num_heads, seq_len, d_v)\n",
        "    mask (torch.Tensor): Optional attention mask of shape (batch_size, 1, 1, seq_len)\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: The attention output of shape (batch_size, num_heads, seq_len, d_v)\n",
        "    \"\"\"\n",
        "    d_k = Q.size(-1)\n",
        "    # <YOUR CODE HERE>\n",
        "\n",
        "    # <END YOUR CODE>\n",
        "\n",
        "# Test the attention function with some random values\n",
        "\n",
        "batch_size = 20\n",
        "seq_len = 5\n",
        "d_k = 512\n",
        "d_v = 64\n",
        "\n",
        "Q = torch.rand(batch_size, seq_len, d_k)\n",
        "K = torch.rand(batch_size, seq_len, d_k)\n",
        "V = torch.rand(batch_size, seq_len, d_v)\n",
        "\n",
        "attention_output, attention_weights = att_score(Q, K, V)\n",
        "attention_output_ref = F.scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "print(f\"Attention Output Shape:\", attention_output.shape)\n",
        "print(f\"Attention Weights Shape:\", attention_weights.shape)\n",
        "\n",
        "print(f\"Is the implementation close to PyTorch implementation? \" f'{torch.allclose(attention_output,attention_output_ref, atol = 1e-3)}')"
      ],
      "metadata": {
        "id": "4qkQPFNFrkQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 2. Masking Mechanism (10 points)\n",
        "\n",
        "In certain scenarios, we need to prevent the model from \"seeing\" future tokens, which ensures that the predictions are only based on the previous tokens in the sequence. For instance, When predicting the next word in a sequence, we don't want the model to attend to future words, as it would give the model access to information it shouldn't have.\n",
        "\n",
        "In this task, you will implement and test a lower-triangular mask, which masks out upper-triangular entries of the score matrix. Notice that the lower-triangular mask is applied before the softmax function, i.e., on $\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right)$. Accordingly, you will need to modify the `attention_score` function in task 1 to take into account such change.\n",
        "\n",
        "Hint: you may use the `masked_fill` function to mask out lower-triangular entries. Try to use a boolean mask in the implementation.\n",
        "\n",
        "If implemented correctly, the output should be like:\n",
        "\n",
        "Is the implementation close to PyTorch implementation?: True\n"
      ],
      "metadata": {
        "id": "ScdFiFz5zsZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(seq_len):\n",
        "  # <YOUR CODE HERE>\n",
        "\n",
        "  # <END YOUR CODE>\n",
        "\n",
        "def attention_score(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Computes scaled dot-product attention.\n",
        "\n",
        "    Parameters:\n",
        "    Q (torch.Tensor): Query matrix of shape (batch_size, num_heads, seq_len, d_k)\n",
        "    K (torch.Tensor): Key matrix of shape (batch_size, num_heads, seq_len, d_k)\n",
        "    V (torch.Tensor): Value matrix of shape (batch_size, num_heads, seq_len, d_v)\n",
        "    mask (torch.Tensor): Optional attention mask of shape (batch_size, 1, 1, seq_len)\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: The attention output of shape (batch_size, num_heads, seq_len, d_v)\n",
        "    \"\"\"\n",
        "    d_k = Q.size(-1)\n",
        "    # <YOUR CODE HERE>\n",
        "\n",
        "    # <END YOUR CODE>\n",
        "\n",
        "# Test the masked attention function\n",
        "\n",
        "mask = create_mask(seq_len=5)\n",
        "attention_output, attention_weights = attention_score(Q, K, V, mask=mask)\n",
        "attention_output_ref = F.scaled_dot_product_attention(Q, K, V, attn_mask=mask)\n",
        "\n",
        "print(f\"Is the implementation close to PyTorch implementation? \" f'{torch.allclose(attention_output,attention_output_ref, atol = 1e-3)}')"
      ],
      "metadata": {
        "id": "CzuJt6fg05qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 3: Self-Attention Module (20 points)\n",
        "\n"
      ],
      "metadata": {
        "id": "-59t7pOen3gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a self-attention module that takes $X$ and computes $Q$, $K$ and $V$ using three linear layers. If implemented correctly, the output should be like:\n",
        "\n",
        "Output shape: torch.Size([20, 5, 64])\n",
        "\n",
        "Output shape: torch.Size([20, 5, 64])\n",
        "Are the outputs permutation-equivalent? True\n",
        "\n",
        "The following two lines should be identical:\n",
        "\n",
        "tensor([-0.0166, -0.4199, -0.5860,  0.3721,  0.5411,  0.4073,  0.8418,  0.2554,\n",
        "        -0.2712,  0.6178, -0.0495, -0.0552,  0.6988, -0.3643,  0.7696],\n",
        "       grad_fn=<SliceBackward0>)\n",
        "       \n",
        "tensor([-0.0166, -0.4199, -0.5860,  0.3721,  0.5411,  0.4073,  0.8418,  0.2554,\n",
        "        -0.2712,  0.6178, -0.0495, -0.0552,  0.6988, -0.3643,  0.7696],\n",
        "       grad_fn=<SliceBackward0>)"
      ],
      "metadata": {
        "id": "Spld39wV7a7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(torch.nn.Module):\n",
        "    def __init__(self, d_in, d_k, d_v):\n",
        "        \"\"\"\n",
        "        Initialize the Self-Attention module.\n",
        "\n",
        "        Parameters:\n",
        "        d_in (int): Dimensionality of the input sequence.\n",
        "        d_k (int): Dimensionality of the queries and keys.\n",
        "        d_v (int): Dimensionality of the values.\n",
        "        \"\"\"\n",
        "        super(SelfAttention, self).__init__()\n",
        "        # <YOUR CODE HERE>\n",
        "\n",
        "        # <END YOUR CODE>\n",
        "\n",
        "    def forward(self, X, mask=None):\n",
        "        \"\"\"\n",
        "        Compute the self-attention for the input sequence.\n",
        "\n",
        "        Parameters:\n",
        "        X (torch.Tensor): Input sequence of shape (batch_size, seq_len, d_model)\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor: Output sequence of shape (batch_size, seq_len, d_model)\n",
        "        torch.Tensor: Attention weights of shape (batch_size, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "        # <YOUR CODE HERE>\n",
        "\n",
        "        # <END YOUR CODE>\n",
        "\n",
        "# Test the self-attention module\n",
        "\n",
        "torch.manual_seed(40)\n",
        "batch_size = 20\n",
        "seq_len = 5\n",
        "d_in = 32\n",
        "d_k = 512\n",
        "d_v = 64\n",
        "\n",
        "X = torch.ones(batch_size, seq_len, d_in)\n",
        "self_attention = SelfAttention(d_in, d_k, d_v)\n",
        "output = self_attention(X)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n",
        "\n",
        "# Check the permutation-equivalent property of self-attention\n",
        "perm = torch.randperm(seq_len)\n",
        "\n",
        "permuted_X = X[:, perm, :]\n",
        "\n",
        "# Forward pass with permuted input\n",
        "permuted_output = self_attention(permuted_X)\n",
        "\n",
        "# Apply the same permutation to the original output to compare\n",
        "repermuted_output = output[:, perm, :]\n",
        "\n",
        "# Check if the outputs are the same (up to numerical precision)\n",
        "are_outputs_equivalent = torch.allclose(permuted_output, repermuted_output, atol=1e-3)\n",
        "\n",
        "print(\"Are the outputs permutation-equivalent?\", are_outputs_equivalent)\n",
        "print(\"The following two lines should be identical:\")\n",
        "print(permuted_output[-1,1,:15])\n",
        "print(repermuted_output[-1,1,:15])"
      ],
      "metadata": {
        "id": "tJban62L7HBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 4: Multi-head Self-Attention Module (30 points)\n",
        "\n",
        "Self-attention mechanisms allow a model to focus on different parts of the input sequence while processing each element. In the multi-head attention setup, multiple attention heads are computed in parallel, and their results are concatenated, which allows the model to capture information from different subspaces. Specifically, multi-head self-attention passes the input to each attention module, concatenates the outputs, and apply a linear layer to obtain the final output:\n",
        "$$\n",
        "\\operatorname{MultiHead}(X)=\\operatorname{Concat}\\left(\\text {head}_1, \\text {head}_2, \\ldots, \\text {head}_h \\right) W^O,\n",
        "$$\n",
        "$$\n",
        "\\operatorname{head}_i=\\operatorname{Attention}(X),\n",
        "$$\n",
        "where $\\operatorname{Attention}$ corresponds to the self-attention module where the dimensions of key, query and values are given as $\\frac{d_k}{h}$, $\\frac{d_k}{h}$ and $\\frac{d_v}{h}$ respectively. Notice that the weights are not necessarily identical for different heads; instead, you may save the attention heads as a `nn.ModuleList`.\n",
        "\n",
        "If implemented correctly, the output should be like:\n",
        "\n",
        "tensor([ 0.0275, -0.1630,  0.1510,  0.3983, -0.0111, -0.4692, -0.1411,  0.1411,\n",
        "         0.2040,  0.0587,  0.1210, -0.1313, -0.0088,  0.1773, -0.0538],\n",
        "       grad_fn=<SliceBackward0>)\n",
        "Output shape: torch.Size([32, 10, 64])"
      ],
      "metadata": {
        "id": "C13rckw8oKyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_k, d_v, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        # <YOUR CODE HERE>\n",
        "\n",
        "        # <END YOUR CODE>\n",
        "\n",
        "    def forward(self, X, mask=None):\n",
        "        # <YOUR CODE HERE>\n",
        "\n",
        "        # <END YOUR CODE>\n",
        "\n",
        "\n",
        "# Testing Multi-Head Attention\n",
        "torch.manual_seed(40)\n",
        "batch_size = 32\n",
        "seq_len = 10\n",
        "d_in = 256\n",
        "d_k = 512\n",
        "d_v = 64\n",
        "num_heads = 8\n",
        "\n",
        "X = torch.rand(batch_size, seq_len, d_in)\n",
        "\n",
        "multi_head_attention = MultiHeadSelfAttention(d_in, d_k, d_v, num_heads)\n",
        "\n",
        "output = multi_head_attention(X)\n",
        "\n",
        "print(output[-1,1,:15])\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "id": "a80mRY5J81LP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}